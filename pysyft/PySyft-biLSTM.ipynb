{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition with federated learning \n",
    "Simple example in implementing a biLSTM model for NER, using PyTorch and PySyft. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This is a work in progress. Currently having some trouble getting federated LSTM to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "from tqdm.notebook import tqdm\n",
    "import syft as sy\n",
    "\n",
    "import torch.nn as nn\n",
    "#from syft.frameworks.torch.nn import LSTM\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Define filepaths and selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path\n",
    "    train_path = \"./data/eng.train\",\n",
    "    embeddings_path = \"./data/glove.840B.300d.txt\", \n",
    "    pad_token = \"<PAD>\",\n",
    "    unk_token = \"<UNK>\",\n",
    "    batch_size=32,\n",
    "    \n",
    "    # Model \n",
    "    embedding_dim = 300,\n",
    "    lstm_dim = 10, #todo\n",
    "    num_lstm_layers = 1,\n",
    "    \n",
    "    # Training\n",
    "    seed=42,\n",
    "    num_epochs=30,\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c1894d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Setup PySyft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "carol = sy.VirtualWorker(hook, id=\"carol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Prepare data\n",
    "CoNLL 2003: https://www.clips.uantwerpen.be/conll2003/ner/\n",
    "* This notebook only uses the English data.\n",
    "* The raw data are a series of text files.\n",
    "    * Each line looks like this: \"EU NNP I-NP I-ORG\"\n",
    "    * Here we are only interested in the actual word (\"EU\") and the NER tag (\"I-ORG\")\n",
    "* Process raw file into tensors, then into federated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filepath, is_cased=True):\n",
    "    \"\"\" Process data in the CoNLL-2003 format into words/labels per sentence. \"\"\"\n",
    "    sents, labels = [], []\n",
    "    curr_sent_words, curr_sent_labels = [], []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if not is_cased:\n",
    "            line = line.lower()\n",
    "        if line == \"\\n\":\n",
    "            sents.append(curr_sent_words)\n",
    "            labels.append(curr_sent_labels)\n",
    "            curr_sent_words, curr_sent_labels = [], []\n",
    "        else:\n",
    "            elements = line.strip().split(\" \")\n",
    "            word = elements[0]\n",
    "            label = elements[-1]\n",
    "            curr_sent_words.append(word)\n",
    "            curr_sent_labels.append(label)\n",
    "    \n",
    "    return sents, labels\n",
    "\n",
    "\n",
    "def process_data(sents, labels, word2id, label2id, max_length, pad_token, unknown_token,\n",
    "                 is_train=True):\n",
    "    \"\"\" Processes data in the CoNLL 2003 format into torch tensors.\n",
    "    \n",
    "    Args:\n",
    "      sents:         List of sentences (each a list of words)\n",
    "      labels:        List of list of labels \n",
    "      word2id:       Dictionary mapping words to int indices\n",
    "      label2id:      Dictionary mapping labels to int indices\n",
    "      max_length:    Target length that we want to pad sequences to.\n",
    "      pad_token:     String for padding.\n",
    "      unknown_token: String to replace OOV words.      \n",
    "      is_train:      Is processing for training data or not. \n",
    "                     If true, updates the dictionaries with each value.\n",
    "                     If false, unseen words will be mapped to <UNK>.\n",
    "    \n",
    "    Returns:\n",
    "      word2id:       Word-to-indices mapping. If is_train, it will be updatd.\n",
    "      label2id:      Label-to-indices mapping. If is_train, it will be updated. \n",
    "      X:             Tensor of processed sentences.\n",
    "      y:             Tensor of processed labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    if is_train:\n",
    "        pad_id, unk_id = 0, 1\n",
    "        word2id[pad_token] = pad_id\n",
    "        label2id[pad_token] = pad_id\n",
    "        word2id[unknown_token] = unk_id\n",
    "    else:\n",
    "        pad_id = word2id[pad_token]\n",
    "        unk_id = word2id[unknown_token]\n",
    "    \n",
    "    for i, sent in enumerate(sents):\n",
    "        X_i, y_i = [], []\n",
    "        curr_labels = labels[i]\n",
    "        sent = sent[:max_length]\n",
    "        diff = max_length - len(sent)\n",
    "        for word, label in zip(sent, curr_labels):\n",
    "            if is_train:\n",
    "                # If preparing training data, update mappings as we go\n",
    "                word_id = word2id.get(word, None)\n",
    "                if not word_id:\n",
    "                    word_id = len(word2id)\n",
    "                    word2id[word] = word_id\n",
    "                \n",
    "                label_id = label2id.get(label, None)\n",
    "                if not label_id:\n",
    "                    label_id = len(label2id)\n",
    "                    label2id[label] = label_id\n",
    "            else:\n",
    "                # Otherwise, fetch id from existing mappings\n",
    "                word_id = word2id.get(word, word2id[unknown_token])\n",
    "                label_id = label2id.get(label)\n",
    "                if not label_id:\n",
    "                    raise LookupError(f'Unseen label {label}')\n",
    "            X_i.append(word_id)\n",
    "            y_i.append(label_id) \n",
    "            \n",
    "        # Pad sequences to max length\n",
    "        X_i.extend([pad_id] * diff)\n",
    "        y_i.extend([pad_id] * diff)\n",
    "        \n",
    "        X.append(X_i)\n",
    "        y.append(y_i)\n",
    "            \n",
    "    X = torch.tensor(X)   \n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    return word2id, label2id, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, labels = extract_sentences(args.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get maximum sequence length based on loaded sentences\n",
    "max_train_sent_length = max(len(sent) for sent in sents)\n",
    "max_length = 2**math.ceil(math.log2(max_train_sent_length))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, label2id = dict(), dict()\n",
    "word2id, label2id, X, y = process_data(\n",
    "    sents=sents, labels=labels, word2id=word2id, label2id=label2id, \n",
    "    max_length=max_length, pad_token=args.pad_token, unknown_token=args.unk_token\n",
    ")\n",
    "vocab_size = len(word2id)\n",
    "num_labels = len(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14986, 128]), torch.Size([14986, 128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform this into PySyft's federated dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "base = sy.BaseDataset(X, y)\n",
    "federated_dataset = base.federate((alice, bob))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_loader = sy.FederatedDataLoader(\n",
    "    federated_dataset=federated_dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    shuffle=False                                   # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed words\n",
    "* Pre-trained word embeddings. Here we use common crawl glove cased, 840B/300d. \n",
    "    * Source: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load pretrained embeddings as frozen weights\n",
    "if False: \n",
    "    word2vec_path = './word2vec.pkl'\n",
    "    if os.path.exists(word2vec_path):\n",
    "        with open(word2vec_path, 'rb') as f:\n",
    "            word2vec = pickle.load(f)\n",
    "    else:\n",
    "        word2vec = dict()\n",
    "        with open(args.embeddings_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in tqdm(lines):        \n",
    "            split_line = line.split(\" \")\n",
    "            word = split_line[0]\n",
    "            vec = torch.tensor([float(v) for v in split_line[1:]])\n",
    "\n",
    "            word2vec[word] = vec\n",
    "        word2vec[args.unk_token] = torch.randn(300)\n",
    "        #word2vec[args.unk_token] = torch.mean(, axis=0) # TODO: replace with mean\n",
    "        word2vec_path = './word2vec.pkl'\n",
    "        with open(word2vec_path, 'wb') as f:\n",
    "            pickle.dump(word2vec, f)\n",
    "    #pretrained_embeddings = torch.zeros((vocab_size, args.embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biLSTM(nn.Module):\n",
    "    def __init__(self, args, word2id, num_labels):\n",
    "        super(biLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(word2id),\n",
    "            embedding_dim=args.embedding_dim,\n",
    "            padding_idx=word2id[args.pad_token]\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(              \n",
    "            input_size=args.embedding_dim,\n",
    "            hidden_size=args.lstm_dim,\n",
    "            num_layers=args.num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            in_features=args.lstm_dim*2,  # bidirectional \n",
    "            out_features=num_labels\n",
    "        ) \n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_embedded = self.embedding(x)\n",
    "        y_out, _ = self.lstm(x_embedded)\n",
    "        \n",
    "        batch_size, max_len, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * max_len, feat_size)\n",
    "        \n",
    "        y_out = self.fc(y_out)\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, max_len, new_feat_size)\n",
    "        \n",
    "        y_out = F.log_softmax(y_out.view(max_len, -1), 1)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status: \n",
    "* Stuck with size issue at LSTM layer.\n",
    "    * Seems to be known issue: e.g., https://github.com/OpenMined/PySyft/issues/3010\n",
    "\n",
    "\n",
    "Details:\n",
    "* I first thought it was the embedding layer's problem, but it turns out even if I 'manually' embedded vectors as input the problem stays the same. \n",
    "* I've also tried (with no luck) with:\n",
    "    * Toy dataset vs actual data\n",
    "    * Federated dataset vs loading data 'manually' and sending components to the workers\n",
    "    * Switching the layer implementations:\n",
    "        * With out-of-the-box PyTorch, I get:\n",
    "            * RuntimeError: input.size(-1) must be equal to input_size. Expected 300, got 0\n",
    "        * With the Syft implementation, I get:\n",
    "            * The expanded size of the tensor (40) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [16, 40].  Tensor sizes: [0]\n",
    "* I haven't tried:\n",
    "    * Implementing LSTM from scratch :/\n",
    "    * Different machine/os\n",
    "    * TF instead of PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = biLSTM(args=args, word2id=word2id, num_labels=len(label2id))\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Toy dataset\n",
    "_X = torch.stack([\n",
    "    torch.randint(low=0, high=vocab_size, size=(max_length,)) \n",
    "    for _ in range(args.batch_size)]\n",
    ")\n",
    "_y = torch.stack([\n",
    "        torch.randint(low=0, high=len(label2id), size=(max_length,)) \n",
    "        for _w in range(args.batch_size)\n",
    "    ])\n",
    "\n",
    "is_federated = False\n",
    "if is_federated:\n",
    "    \n",
    "    alice.clear_objects()\n",
    "    bob.clear_objects()\n",
    "    \n",
    "    base = sy.BaseDataset(_X, _y)\n",
    "    dataset = base.federate((alice, bob))\n",
    "    train_loader = sy.FederatedDataLoader(\n",
    "        federated_dataset=dataset, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=False                                   # <- TODO\n",
    "    )\n",
    "    optimizers = { \n",
    "        worker: optim.Adam(params=model.parameters(),lr=args.lr) \n",
    "        for worker in dataset.workers\n",
    "    }\n",
    "else:\n",
    "    #dataset = torch.utils.data.TensorDataset(_X, _y)\n",
    "    dataset = torch.utils.data.TensorDataset(X[:100], y[:100])\n",
    "    train_loader = torch.utils.data.DataLoader(dataset)\n",
    "    optimizer = optim.Adam(params=model.parameters(),lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.401567220687866\n",
      "2.3967275619506836\n",
      "2.3915979862213135\n",
      "2.3219316005706787\n",
      "2.3378689289093018\n",
      "2.311460018157959\n",
      "2.328676223754883\n",
      "2.281559467315674\n",
      "2.3150763511657715\n",
      "2.2749524116516113\n",
      "2.292695999145508\n",
      "2.320054292678833\n",
      "2.2991244792938232\n",
      "2.2656478881835938\n",
      "2.255303144454956\n",
      "2.2499170303344727\n",
      "2.224112033843994\n",
      "2.2396445274353027\n",
      "2.2546756267547607\n",
      "2.250839948654175\n",
      "2.255213737487793\n",
      "2.238948345184326\n",
      "2.230776309967041\n",
      "2.207616090774536\n",
      "2.2005298137664795\n",
      "2.1686594486236572\n",
      "2.1926522254943848\n",
      "2.1606662273406982\n",
      "2.174736499786377\n",
      "2.1863174438476562\n",
      "2.172788381576538\n",
      "2.1629626750946045\n",
      "2.1268396377563477\n",
      "2.1052889823913574\n",
      "2.12015700340271\n",
      "2.11755108833313\n",
      "2.0891902446746826\n",
      "2.0863113403320312\n",
      "2.0802483558654785\n",
      "2.1063225269317627\n",
      "2.0888566970825195\n",
      "2.082275152206421\n",
      "2.041229724884033\n",
      "2.036936044692993\n",
      "2.062511682510376\n",
      "2.052417516708374\n",
      "2.038029193878174\n",
      "2.032695770263672\n",
      "2.011843681335449\n",
      "2.011183023452759\n",
      "2.004842519760132\n",
      "1.9568209648132324\n",
      "1.9900898933410645\n",
      "1.95694100856781\n",
      "1.968299150466919\n",
      "1.951612949371338\n",
      "1.9307081699371338\n",
      "1.9223006963729858\n",
      "1.898830533027649\n",
      "1.9009374380111694\n",
      "1.8974151611328125\n",
      "1.8690372705459595\n",
      "1.8836431503295898\n",
      "1.8409223556518555\n",
      "1.8451716899871826\n",
      "1.8158124685287476\n",
      "1.8098064661026\n",
      "1.7786377668380737\n",
      "1.7759007215499878\n",
      "1.756105899810791\n",
      "1.750402808189392\n",
      "1.7399996519088745\n",
      "1.728704571723938\n",
      "1.698411226272583\n",
      "1.7095423936843872\n",
      "1.6580692529678345\n",
      "1.6430867910385132\n",
      "1.6286479234695435\n",
      "1.5918627977371216\n",
      "1.5722317695617676\n",
      "1.556124210357666\n",
      "1.4899338483810425\n",
      "1.505900263786316\n",
      "1.4326149225234985\n",
      "1.4620333909988403\n",
      "1.4282597303390503\n",
      "1.4074970483779907\n",
      "1.3653947114944458\n",
      "1.3416727781295776\n",
      "1.27959144115448\n",
      "1.2474223375320435\n",
      "1.2967283725738525\n",
      "1.225701093673706\n",
      "1.1899116039276123\n",
      "1.1696407794952393\n",
      "1.1080307960510254\n",
      "1.0887761116027832\n",
      "0.9995306730270386\n",
      "1.034424901008606\n",
      "1.0111585855484009\n",
      "0.9501911997795105\n",
      "0.8702979683876038\n",
      "0.8373712301254272\n",
      "1.0169868469238281\n",
      "1.0158339738845825\n",
      "1.013542652130127\n",
      "0.9177337884902954\n",
      "1.013571858406067\n",
      "0.9296539425849915\n",
      "0.9470024108886719\n",
      "0.8629572987556458\n",
      "0.5715808272361755\n",
      "0.6293901205062866\n",
      "0.7670862078666687\n",
      "0.7886584401130676\n",
      "0.8395077586174011\n",
      "0.8714274764060974\n",
      "0.8060631155967712\n",
      "0.5826300382614136\n",
      "0.5613921284675598\n",
      "0.3964691162109375\n",
      "0.47682780027389526\n",
      "0.36901962757110596\n",
      "0.7040050029754639\n",
      "0.7164275646209717\n",
      "0.6083574891090393\n",
      "0.6521393656730652\n",
      "0.649739682674408\n",
      "0.3860050439834595\n",
      "0.2720656991004944\n",
      "0.3367456793785095\n",
      "0.2611192762851715\n",
      "0.6212273836135864\n",
      "0.7258518934249878\n",
      "0.5859881639480591\n",
      "0.3435826301574707\n",
      "0.4372493326663971\n",
      "0.6814391016960144\n",
      "0.5217723250389099\n",
      "0.18860206007957458\n",
      "0.25535041093826294\n",
      "0.1890040636062622\n",
      "0.3931734263896942\n",
      "0.6149049401283264\n",
      "0.1612502485513687\n",
      "0.26540035009384155\n",
      "0.16631314158439636\n",
      "0.4550201892852783\n",
      "0.32217952609062195\n",
      "0.3294259011745453\n",
      "0.2120688408613205\n",
      "0.4014261066913605\n",
      "0.2679190933704376\n",
      "0.2549681067466736\n",
      "0.34285324811935425\n",
      "0.2229510396718979\n",
      "0.2774731516838074\n",
      "0.22832785546779633\n",
      "0.1735171526670456\n",
      "0.11038985103368759\n",
      "0.21814608573913574\n",
      "0.12205363810062408\n",
      "0.4282897412776947\n",
      "0.305406779050827\n",
      "0.20711937546730042\n",
      "0.09772492945194244\n",
      "0.18243969976902008\n",
      "0.10825753211975098\n",
      "0.2057446986436844\n",
      "0.13321858644485474\n",
      "0.18992187082767487\n",
      "0.18923969566822052\n",
      "0.19710379838943481\n",
      "0.17195235192775726\n",
      "0.2749894857406616\n",
      "0.17724373936653137\n",
      "0.18962953984737396\n",
      "0.18529826402664185\n",
      "0.16309425234794617\n",
      "0.17739123106002808\n",
      "0.17191341519355774\n",
      "0.07474710792303085\n",
      "0.17801646888256073\n",
      "0.08690762519836426\n",
      "0.2215721160173416\n",
      "0.18918509781360626\n",
      "0.19256076216697693\n",
      "0.16904497146606445\n",
      "0.16862475872039795\n",
      "0.11401595175266266\n",
      "0.11288730055093765\n",
      "0.2672925889492035\n",
      "0.16887065768241882\n",
      "0.1759319007396698\n",
      "0.176457479596138\n",
      "0.129485622048378\n",
      "0.14911891520023346\n",
      "0.06082116439938545\n",
      "0.16411727666854858\n",
      "0.17375412583351135\n",
      "0.13917453587055206\n",
      "0.08069291710853577\n",
      "0.07792634516954422\n",
      "0.329782634973526\n",
      "0.3639748990535736\n",
      "0.40221017599105835\n",
      "0.29020312428474426\n",
      "0.4024800658226013\n",
      "0.37328675389289856\n",
      "0.3727477788925171\n",
      "0.34936296939849854\n",
      "0.0473749004304409\n",
      "0.14276957511901855\n",
      "0.26201850175857544\n",
      "0.3062078654766083\n",
      "0.37545517086982727\n",
      "0.46181705594062805\n",
      "0.38078972697257996\n",
      "0.19643990695476532\n",
      "0.18309006094932556\n",
      "0.04865286126732826\n",
      "0.13344989717006683\n",
      "0.06261643022298813\n",
      "0.3283722400665283\n",
      "0.3788738548755646\n",
      "0.25497040152549744\n",
      "0.3288913071155548\n",
      "0.3441130518913269\n",
      "0.12020734697580338\n",
      "0.04511630907654762\n",
      "0.11271931976079941\n",
      "0.06305450946092606\n",
      "0.3566308319568634\n",
      "0.41669514775276184\n",
      "0.30432161688804626\n",
      "0.1196608617901802\n",
      "0.20991623401641846\n",
      "0.3994407057762146\n",
      "0.2994118928909302\n",
      "0.04177071526646614\n",
      "0.0981396734714508\n",
      "0.05926593020558357\n",
      "0.18470542132854462\n",
      "0.39294663071632385\n",
      "0.040254175662994385\n",
      "0.10793853551149368\n",
      "0.058985475450754166\n",
      "0.2626001536846161\n",
      "0.13987234234809875\n",
      "0.1392054259777069\n",
      "0.08698814362287521\n",
      "0.19861778616905212\n",
      "0.1248660609126091\n",
      "0.12156087160110474\n",
      "0.19329987466335297\n",
      "0.10200033336877823\n",
      "0.1391485333442688\n",
      "0.09954330325126648\n",
      "0.07138828188180923\n",
      "0.036188989877700806\n",
      "0.11169078946113586\n",
      "0.05448753759264946\n",
      "0.27260300517082214\n",
      "0.16816207766532898\n",
      "0.13628244400024414\n",
      "0.0347210168838501\n",
      "0.09971781075000763\n",
      "0.049049731343984604\n",
      "0.118358314037323\n",
      "0.07605850696563721\n",
      "0.1017022654414177\n",
      "0.09938599914312363\n",
      "0.11606591194868088\n",
      "0.09350074827671051\n",
      "0.17864461243152618\n",
      "0.08065067231655121\n",
      "0.10686712712049484\n",
      "0.09947248548269272\n",
      "0.09214238822460175\n",
      "0.12148746103048325\n",
      "0.09997475147247314\n",
      "0.03126927465200424\n",
      "0.10145366936922073\n",
      "0.04538118094205856\n",
      "0.14615531265735626\n",
      "0.1265089362859726\n",
      "0.10575592517852783\n",
      "0.10327386856079102\n",
      "0.08935534954071045\n",
      "0.0628591999411583\n",
      "0.05604007840156555\n",
      "0.18517103791236877\n",
      "0.09077052026987076\n",
      "0.11847995966672897\n",
      "0.09463508427143097\n",
      "0.08134336024522781\n",
      "0.09221900999546051\n",
      "0.028427353128790855\n",
      "0.10921477526426315\n",
      "0.12756061553955078\n",
      "0.09363758563995361\n",
      "0.058501601219177246\n",
      "0.049937762320041656\n",
      "0.20639900863170624\n",
      "0.24279123544692993\n",
      "0.27488481998443604\n",
      "0.171054407954216\n",
      "0.2355208843946457\n",
      "0.27170702815055847\n",
      "0.20304657518863678\n",
      "0.24887490272521973\n",
      "0.022227037698030472\n",
      "0.10451444238424301\n",
      "0.14382445812225342\n",
      "0.16802075505256653\n",
      "0.22697646915912628\n",
      "0.3365813195705414\n",
      "0.23186597228050232\n",
      "0.12564286589622498\n",
      "0.1104375347495079\n",
      "0.025137294083833694\n",
      "0.08027605712413788\n",
      "0.03945343196392059\n",
      "0.19525153934955597\n",
      "0.25989052653312683\n",
      "0.14481985569000244\n",
      "0.19901102781295776\n",
      "0.21623265743255615\n",
      "0.06340640038251877\n",
      "0.02389569953083992\n",
      "0.07480576634407043\n",
      "0.042605116963386536\n",
      "0.2611778676509857\n",
      "0.2733117640018463\n",
      "0.1910347044467926\n",
      "0.058949895203113556\n",
      "0.13926702737808228\n",
      "0.25459223985671997\n",
      "0.20273898541927338\n",
      "0.022629382088780403\n",
      "0.0672905296087265\n",
      "0.0410565510392189\n",
      "0.10417356342077255\n",
      "0.3008207678794861\n",
      "0.02201661840081215\n",
      "0.060034994035959244\n",
      "0.04197908192873001\n",
      "0.18675903975963593\n",
      "0.06513896584510803\n",
      "0.06514803320169449\n",
      "0.04962781071662903\n",
      "0.11860867589712143\n",
      "0.07473358511924744\n",
      "0.07773111760616302\n",
      "0.13649030029773712\n",
      "0.06089622899889946\n",
      "0.08814358711242676\n",
      "0.057450857013463974\n",
      "0.0438472218811512\n",
      "0.020420704036951065\n",
      "0.06804927438497543\n",
      "0.03902621939778328\n",
      "0.18677914142608643\n",
      "0.11043699085712433\n",
      "0.11532598733901978\n",
      "0.019836660474538803\n",
      "0.07120732963085175\n",
      "0.034270524978637695\n",
      "0.0804794654250145\n",
      "0.06113254651427269\n",
      "0.0634571760892868\n",
      "0.06025860458612442\n",
      "0.07246281206607819\n",
      "0.05549179017543793\n",
      "0.1322840452194214\n",
      "0.045286186039447784\n",
      "0.07101868838071823\n",
      "0.06162706017494202\n",
      "0.05685168877243996\n",
      "0.09227840602397919\n",
      "0.07109309732913971\n",
      "0.018443958833813667\n",
      "0.06378913670778275\n",
      "0.03259945660829544\n",
      "0.10801579803228378\n",
      "0.09888411313295364\n",
      "0.05677890032529831\n",
      "0.07144371420145035\n",
      "0.05268352851271629\n",
      "0.042543668299913406\n",
      "0.03438398987054825\n",
      "0.14024603366851807\n",
      "0.05439942702651024\n",
      "0.0885930210351944\n",
      "0.04675854369997978\n",
      "0.05036889761686325\n",
      "0.06703423708677292\n",
      "0.017257198691368103\n",
      "0.08218153566122055\n",
      "0.10442276298999786\n",
      "0.0747515931725502\n",
      "0.05251539871096611\n",
      "0.03973053768277168\n",
      "0.1411834955215454\n",
      "0.1781369298696518\n",
      "0.19735585153102875\n",
      "0.10717619210481644\n",
      "0.14716404676437378\n",
      "0.21585369110107422\n",
      "0.11869046837091446\n",
      "0.18533991277217865\n",
      "0.013457030989229679\n",
      "0.08477283269166946\n",
      "0.08611021935939789\n",
      "0.10180189460515976\n",
      "0.15673799812793732\n",
      "0.2701595723628998\n",
      "0.13950899243354797\n",
      "0.08947347104549408\n",
      "0.073328398168087\n",
      "0.015869140625\n",
      "0.05417857691645622\n",
      "0.02965003252029419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12757723033428192\n",
      "0.20822715759277344\n",
      "0.09565652906894684\n",
      "0.1372678577899933\n",
      "0.1433940827846527\n",
      "0.036021482199430466\n",
      "0.015365582890808582\n",
      "0.05733690783381462\n",
      "0.03325024992227554\n",
      "0.2138832062482834\n",
      "0.20989006757736206\n",
      "0.14087052643299103\n",
      "0.03496502339839935\n",
      "0.1119387224316597\n",
      "0.18437440693378448\n",
      "0.1529606282711029\n",
      "0.014853940345346928\n",
      "0.053554367274045944\n",
      "0.03234286606311798\n",
      "0.06867146492004395\n",
      "0.2584233582019806\n",
      "0.01460261084139347\n",
      "0.03834372013807297\n",
      "0.03379980847239494\n",
      "0.14894051849842072\n",
      "0.03523169830441475\n",
      "0.0343535915017128\n",
      "0.028618214651942253\n",
      "0.08161397278308868\n",
      "0.05013946443796158\n",
      "0.05824936926364899\n",
      "0.1089237853884697\n",
      "0.044493384659290314\n",
      "0.06872252374887466\n",
      "0.040611691772937775\n",
      "0.03271442651748657\n",
      "0.013867395929992199\n",
      "0.047226738184690475\n",
      "0.03199184685945511\n",
      "0.13978931307792664\n",
      "0.08212792873382568\n",
      "0.10048885643482208\n",
      "0.013571717776358128\n",
      "0.05665452778339386\n",
      "0.026988845318555832\n",
      "0.06389883905649185\n",
      "0.053573984652757645\n",
      "0.04428631067276001\n",
      "0.04052726924419403\n",
      "0.043422993272542953\n",
      "0.03137896582484245\n",
      "0.10693524777889252\n",
      "0.02957255020737648\n",
      "0.05201302841305733\n",
      "0.035854145884513855\n",
      "0.03513961657881737\n",
      "0.07294624298810959\n",
      "0.05617333576083183\n",
      "0.01281246729195118\n",
      "0.04620042443275452\n",
      "0.026018518954515457\n",
      "0.08794176578521729\n",
      "0.08183391392230988\n",
      "0.033675529062747955\n",
      "0.050935760140419006\n",
      "0.033302657306194305\n",
      "0.03216388076543808\n",
      "0.023291204124689102\n",
      "0.11307568848133087\n",
      "0.035075969994068146\n",
      "0.06942974776029587\n",
      "0.026165612041950226\n",
      "0.03398384526371956\n",
      "0.05216694623231888\n",
      "0.012129183858633041\n",
      "0.06874672323465347\n",
      "0.08930851519107819\n"
     ]
    }
   ],
   "source": [
    "# Attempts at getting syft to work. \n",
    "# Snippet is far from complete, but it doesn't matter as \n",
    "# the attempts at using Syft generally fails in the forward pass, \n",
    "# at self.lstm(x_embedded) stage.\n",
    "\n",
    "for epoch in range(args.num_epochs)[:5]:  # <- TODO\n",
    "\n",
    "    for data, label in train_loader:\n",
    "        \n",
    "        if is_federated:                   # <- size problem with federated dataset\n",
    "            worker = data.location.id\n",
    "            model.send(worker)\n",
    "        \n",
    "            optimizers[worker].zero_grad()\n",
    "        elif False:                        # <- attempt 2; size problem with manual/'vanilla' syft\n",
    "            model.send(bob)\n",
    "            data_ptr = data.send(bob)\n",
    "            label_ptr = label.send(bob)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_probs = model(data_ptr)\n",
    "            loss = criterion(log_probs, label_ptr.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.get() \n",
    "        else:                               # regular pytorch runs with no problems\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(data)\n",
    "            loss = criterion(log_probs, label.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
