{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cTXNuJrI-vf"
   },
   "source": [
    "# Example: Federated named entity recognition\n",
    "\n",
    "This notebook goes through data preparation and distributed learning for NER using PyTorch and [PySyft](https://github.com/OpenMined/PySyft). \n",
    "\n",
    "Note that we implement a vanilla biLSTM for the time being since there's an outstanding library issue. ([Minimal example + link to issue](https://github.com/j-chim/nlp-examples/blob/pysyft-ner/pysyft/minimal-example.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKaXvwhfJHuD"
   },
   "source": [
    "# Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fv_0RvKrJCTB",
    "outputId": "89cbaa9d-bc3f-4527-af6f-ef3087406689"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tf-encrypted\n",
    "!pip install 'syft[udacity]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "XIUBnbBuI-vh",
    "outputId": "409bbf5e-dfc4-4402-8733-ec9bcf97e4c9"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import syft as sy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEmGehRO21RW"
   },
   "source": [
    "Define filepaths and selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMrevMmow1Mz"
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_path = \"./data/eng.train\",\n",
    "    embeddings_path = \"./data/glove.840B.300d.txt\", \n",
    "    pad_token = \"<PAD>\",\n",
    "    unk_token = \"<UNK>\",\n",
    "    batch_size = 32,\n",
    "    \n",
    "    # Model \n",
    "    embedding_dim = 300,\n",
    "    lstm_dim = 128,\n",
    "    \n",
    "    # Training\n",
    "    seed = 42,\n",
    "    num_epochs = 30,\n",
    "    lr = 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfGD_LbL4EaF"
   },
   "outputs": [],
   "source": [
    "assert(os.path.exists(args.train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jd1y9PMEw-aK",
    "outputId": "1b96bb2f-95b2-4cc6-cedb-212a7eafd0ad"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed) # set random seed with args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njC-7jvs82J2"
   },
   "source": [
    "# Step 1: Create embeddings + federated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsVU8m5w-d6O"
   },
   "source": [
    "The following cell downloads pretrained word embeddings. It may take a few minutes.\n",
    "* For our example we used [common crawl glove cased](https://nlp.stanford.edu/projects/glove/), 840B/300d.\n",
    "\n",
    "We don't provide the training file directly here, but they can be accessed through the [CoNLL 2003 official website](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
    "*   This notebook only uses the English data. The raw data are a series of text files. \n",
    "* Each line looks like this: \"EU NNP I-NP I-ORG\"\n",
    "* Here we are only interested in the actual word (\"EU\") and the NER tag (\"I-ORG\").\n",
    "\n",
    "When setting args later, make sure the paths to the training files and embeddings are set accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEnKMaS8-x-v"
   },
   "source": [
    "a. Process embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "mDYsiQpE-Xl1",
    "outputId": "3135fee8-e1fb-4ff0-c53d-dc6ebdd9f25a"
   },
   "outputs": [],
   "source": [
    "# Download pretrained word embeddings.\n",
    "# We can also skip this and randomly initialise\n",
    "use_pretrained_embeddings = True\n",
    "\n",
    "if use_pretrained_embeddings:\n",
    "  !mkdir -p ./data/\n",
    "  !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "  !unzip compressed_file_name.zip -d ./data/\n",
    "else:\n",
    "    pickled_embeddings_path = './word2vec.pkl'\n",
    "    if os.path.exists(pickled_embeddings_path):\n",
    "        with open(pickled_embeddings_path, 'rb') as f:\n",
    "            vector_mapper = pickle.load(f)\n",
    "    else:\n",
    "        vector_mapper = dict()\n",
    "        with open(args.embeddings_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in tqdm(lines):        \n",
    "            split_line = line.split(\" \")\n",
    "            word = split_line[0]\n",
    "            vec = torch.tensor([float(v) for v in split_line[1:]])\n",
    "\n",
    "            vector_mapper[word] = vec\n",
    "        vector_mapper[args.unk_token] = torch.randn(300) # use mean instead of randn \n",
    "                                                         # for better performance\n",
    "        with open(pickled_embeddings_path, 'wb') as f:\n",
    "            pickle.dump(vector_mapper, f)\n",
    "    \n",
    "    pretrained_embeddings = torch.zeros((vocab_size, args.embedding_dim))\n",
    "    for w, v in tqdm(vector_mapper.items()): \n",
    "        idx = word2id.get(w, word2id[args.unk_token])\n",
    "        pretrained_embeddings[idx,:] = v   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JNX_k2u2xVf"
   },
   "source": [
    "b. Define virtual workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feWWiDXl2SeL"
   },
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "carol = sy.VirtualWorker(hook, id=\"carol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmoRb_B72c4k"
   },
   "source": [
    "c. Prepare data\n",
    "\n",
    "Process raw file into tensors, then into federated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJUjGv5B2aZn"
   },
   "outputs": [],
   "source": [
    "def extract_sentences(filepath, is_cased=True):\n",
    "    \"\"\" Process data in the CoNLL-2003 format into words/labels per sentence. \"\"\"\n",
    "    sents, labels = [], []\n",
    "    curr_sent_words, curr_sent_labels = [], []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if not is_cased:\n",
    "            line = line.lower()\n",
    "        if line == \"\\n\":\n",
    "            sents.append(curr_sent_words)\n",
    "            labels.append(curr_sent_labels)\n",
    "            curr_sent_words, curr_sent_labels = [], []\n",
    "        else:\n",
    "            elements = line.strip().split(\" \")\n",
    "            word = elements[0]\n",
    "            label = elements[-1]\n",
    "            curr_sent_words.append(word)\n",
    "            curr_sent_labels.append(label)\n",
    "    \n",
    "    return sents, labels\n",
    "\n",
    "\n",
    "def process_data(sents, labels, word2id, label2id, max_length, pad_token, unknown_token,\n",
    "                 is_train=True):\n",
    "    \"\"\" Processes data in the CoNLL 2003 format into torch tensors.\n",
    "    \n",
    "    Args:\n",
    "      sents:         List of sentences (each a list of words)\n",
    "      labels:        List of list of labels \n",
    "      word2id:       Dictionary mapping words to int indices\n",
    "      label2id:      Dictionary mapping labels to int indices\n",
    "      max_length:    Target length that we want to pad sequences to.\n",
    "      pad_token:     String for padding.\n",
    "      unknown_token: String to replace OOV words.      \n",
    "      is_train:      Is processing for training data or not. \n",
    "                     If true, updates the dictionaries with each value.\n",
    "                     If false, unseen words will be mapped to <UNK>.\n",
    "    \n",
    "    Returns:\n",
    "      word2id:       Word-to-indices mapping. If is_train, it will be updatd.\n",
    "      label2id:      Label-to-indices mapping. If is_train, it will be updated. \n",
    "      X:             Tensor of processed sentences.\n",
    "      y:             Tensor of processed labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    if is_train:\n",
    "        pad_id, unk_id = 0, 1\n",
    "        word2id[pad_token] = pad_id\n",
    "        label2id[pad_token] = pad_id\n",
    "        word2id[unknown_token] = unk_id\n",
    "    else:\n",
    "        pad_id = word2id[pad_token]\n",
    "        unk_id = word2id[unknown_token]\n",
    "    \n",
    "    for i, sent in enumerate(sents):\n",
    "        X_i, y_i = [], []\n",
    "        curr_labels = labels[i]\n",
    "        sent = sent[:max_length]\n",
    "        diff = max_length - len(sent)\n",
    "        for word, label in zip(sent, curr_labels):\n",
    "            if is_train:\n",
    "                # If preparing training data, update mappings as we go\n",
    "                word_id = word2id.get(word, None)\n",
    "                if not word_id:\n",
    "                    word_id = len(word2id)\n",
    "                    word2id[word] = word_id\n",
    "                \n",
    "                label_id = label2id.get(label, None)\n",
    "                if not label_id:\n",
    "                    label_id = len(label2id)\n",
    "                    label2id[label] = label_id\n",
    "            else:\n",
    "                # Otherwise, fetch id from existing mappings\n",
    "                word_id = word2id.get(word, word2id[unknown_token])\n",
    "                label_id = label2id.get(label)\n",
    "                if not label_id:\n",
    "                    raise LookupError(f'Unseen label {label}')\n",
    "            X_i.append(word_id)\n",
    "            y_i.append(label_id) \n",
    "            \n",
    "        # Pad sequences to max length\n",
    "        X_i.extend([pad_id] * diff)\n",
    "        y_i.extend([pad_id] * diff)\n",
    "        \n",
    "        X.append(X_i)\n",
    "        y.append(y_i)\n",
    "            \n",
    "    X = torch.tensor(X)   \n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    return word2id, label2id, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n_YUp1C8HK4"
   },
   "outputs": [],
   "source": [
    "sents, labels = extract_sentences(args.train_path)\n",
    "_X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "    sents, labels, test_size=0.1, random_state=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_gzRWqu8KuZ"
   },
   "outputs": [],
   "source": [
    "# Get maximum sequence length based on loaded sentences\n",
    "max_train_sent_length = max(len(train_sent) for train_sent in _X_train)\n",
    "max_length = 2**math.ceil(math.log2(max_train_sent_length))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qqtpwlfe8NtQ"
   },
   "outputs": [],
   "source": [
    "word2id, label2id = dict(), dict()\n",
    "word2id, label2id, X_train, y_train = process_data(\n",
    "    sents=_X_train, labels=_y_train, word2id=word2id, label2id=label2id, \n",
    "    max_length=max_length, pad_token=args.pad_token, unknown_token=args.unk_token\n",
    ")\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlwZRUF28SGA"
   },
   "outputs": [],
   "source": [
    "_, _, X_val, y_val = process_data(\n",
    "    sents=_X_val, labels=_y_val, word2id=word2id, label2id=label2id, \n",
    "    max_length=max_length, pad_token=args.pad_token, unknown_token=args.unk_token,\n",
    "    is_train=False\n",
    ")\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NERJsyxD8YS6"
   },
   "outputs": [],
   "source": [
    "train_base = sy.BaseDataset(X_train, y_train) \n",
    "train_dataset = train_base.federate((alice, bob))\n",
    "train_loader = sy.FederatedDataLoader(\n",
    "    federated_dataset=train_dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    shuffle=True                     \n",
    ")\n",
    "\n",
    "val_base = sy.BaseDataset(X_val, y_val) \n",
    "val_dataset = val_base.federate((alice, bob))\n",
    "val_loader = sy.FederatedDataLoader(\n",
    "    federated_dataset=val_dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    shuffle=True                     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71eKkBRPJO7y"
   },
   "source": [
    "# Step 2: Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXUyrVjA_EbX"
   },
   "source": [
    "Normally, this can be replaced with torch.nn.LSTM()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V5SnS7cI-vn"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\" Vanilla LSTM.\n",
    "    Adapted from: https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, \n",
    "                 batch_first=True,  bidirectional=True):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size:    input vector size.\n",
    "            hidden_size:   hidden state dimension.\n",
    "            batch_first:   whether first dimension of inputs is batch_size.\n",
    "            bidirectional: whether to run model in both directions.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # gate values computed using previous hidden state and current input\n",
    "        self.concat_size = self.hidden_size + input_size\n",
    "        \n",
    "         # forget gate\n",
    "        self.f_t = nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.sigmoid_f = nn.Sigmoid()\n",
    "        \n",
    "        # input gate\n",
    "        self.i_t = nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.sigmoid_i = nn.Sigmoid()\n",
    "        \n",
    "        # output gate\n",
    "        self.o_t = nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.sigmoid_o = nn.Sigmoid()\n",
    "        \n",
    "        # candidate cell state\n",
    "        self.c_tilde = nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.tanh_cell = nn.Tanh()\n",
    "        \n",
    "        self.tanh_hidden = nn.Tanh()        \n",
    "        \n",
    "    def init_states(self, x, batch_size):\n",
    "        zeros = torch.zeros(batch_size,\n",
    "                            self.hidden_size,\n",
    "                            dtype=x.dtype, \n",
    "                            device=x.device)\n",
    "        location = x.location\n",
    "        if location is not None:\n",
    "            return (zeros.send(location), zeros.send(location))\n",
    "        return (zeros, zeros)\n",
    "    \n",
    "    def _pass(self, batch_size, x, prev_h, prev_c):\n",
    "        concatenated = torch.cat((prev_h, x), -1)\n",
    "        \n",
    "        input_gate = self.sigmoid_i(self.i_t(concatenated))\n",
    "        forget_gate = self.sigmoid_f(self.f_t(concatenated))\n",
    "        output_gate = self.sigmoid_o(self.o_t(concatenated))\n",
    "\n",
    "        c_tilde = self.tanh_cell(self.c_tilde(concatenated))\n",
    "        cell_state = (forget_gate * prev_c) + (input_gate * c_tilde)\n",
    "        \n",
    "        hidden = output_gate * self.tanh_hidden(cell_state)\n",
    "    \n",
    "        return (hidden, cell_state)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, prev_hidden=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:              input tensor (batch_size, seq_size, feat_size) \n",
    "            prev_hidden:    tuple of initial/previous hidden state and cell state.\n",
    "                            Defaults to zero. \n",
    "        Returns:\n",
    "            hidden:         LSTM hidden state (batch_size, seq_size, hidden_size)\n",
    "            cell_state:     LSTM cell state \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x.shape\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x.shape\n",
    "            x = x.permute(1, 0, 2)             \n",
    "        \n",
    "        if prev_hidden is None:\n",
    "            prev_h, prev_c = self.init_states(x, batch_size)\n",
    "        else:\n",
    "            prev_h, prev_c = prev_hidden\n",
    "        \n",
    "        h_forward, h_backward = [], []\n",
    "        c_forward, c_backward = [], [] \n",
    "        h_0, c_0 = prev_h, prev_c\n",
    "            \n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x[:,t,:]\n",
    "            prev_h, prev_c = self._pass(batch_size, x_t, prev_h, prev_c)\n",
    "            h_forward.append(prev_h)\n",
    "            c_forward.append(prev_c)\n",
    "            \n",
    "        if self.bidirectional:\n",
    "            prev_h, prev_c = h_0, c_0\n",
    "            for t in reversed(range(seq_size)):\n",
    "                x_t = x[:,t,:]\n",
    "                prev_h, prev_c = self._pass(batch_size, x_t, prev_h, prev_c)\n",
    "                h_backward.append(prev_h)\n",
    "                c_backward.append(prev_c)\n",
    "            \n",
    "            h = torch.cat((torch.stack(h_forward), torch.stack(h_backward)), -1)\n",
    "            c = torch.cat((torch.stack(c_forward), torch.stack(c_backward)), -1)\n",
    "        else:\n",
    "            h = torch.stack(h_forward)\n",
    "            c = torch.stack(c_forward)\n",
    "            \n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QnHiuYJ_Bzu"
   },
   "source": [
    "Use LSTM implemented above and combine with embedding + linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ynyNSrj--8T"
   },
   "outputs": [],
   "source": [
    "class Tagger(nn.Module):\n",
    "    \"\"\" biLSTM for sequence tagging. \"\"\"\n",
    "    \n",
    "    def __init__(self, args, vocab_size, num_labels, \n",
    "                 pretrained_embeddings=None, batch_first=True, padding_idx=0):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            args:                  namespace object containing configs.\n",
    "            vocab_size:            number of unique words in training set. \n",
    "            num_labels:            number of output labels.\n",
    "            pretrained_embeddings: pretrained word embedding weights.\n",
    "                                   If None, weights will initialise randomly.\n",
    "            batch_first:           whether first dimension of inputs is batch_size.\n",
    "            padding_idx:           id corresponding to <PAD> tokens.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Tagger, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = args.embedding_dim\n",
    "        self.lstm_hidden_size = args.lstm_dim\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                pretrained_embeddings\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=self.vocab_size,\n",
    "                embedding_dim=self.embedding_size, \n",
    "                padding_idx=padding_idx            \n",
    "            )\n",
    "        \n",
    "        self.lstm = LSTM(\n",
    "            input_size=self.embedding_size,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            batch_first=self.batch_first,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            in_features=self.lstm_hidden_size*2,  # bidirectional \n",
    "            out_features=self.num_labels\n",
    "        ) \n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "                    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_len = x.shape\n",
    "        else:\n",
    "            seq_len, batch_size = x.shape\n",
    "            x = x.permute(1, 0)\n",
    "            \n",
    "        embedded = self.embedding(x)\n",
    "        h, c = self.lstm(embedded)\n",
    "        y_out = self.fc(h)\n",
    "        \n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, new_feat_size, seq_len) \n",
    "        logits = self.log_softmax(y_out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVVwqXpQJT5j"
   },
   "source": [
    "# Step 3: Instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCuKM5fJI-vq"
   },
   "outputs": [],
   "source": [
    "model = Tagger(args=args, vocab_size=len(word2id), num_labels=len(label2id))\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "compute_nodes = [ w for w in train_dataset.workers]\n",
    "models = { n: model.copy() for n in compute_nodes }\n",
    "optimizers = { \n",
    "    n: optim.Adam(params=models[n].parameters(),lr=args.lr) \n",
    "    for n in compute_nodes\n",
    "}\n",
    "params = { n: list(m.parameters()) for n, m in models.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aepSY8XfJXWK"
   },
   "source": [
    "# Step 4: Train over our distributed dataset\n",
    "\n",
    "This roughly follows the example outlined in [PySyft's official tutorial](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb).\n",
    "\n",
    "* Use fixed precision encoding to encode parameters for SMPC\n",
    "* Share encrypted parameters to workers\n",
    "* Update central model by fetching and decrypting updates from remote workers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XTVcIAmC5qb"
   },
   "outputs": [],
   "source": [
    "def update(data, label, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    log_probs = model(data)\n",
    "    loss = criterion(log_probs, label.squeeze())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrqZQmTNI-vx"
   },
   "outputs": [],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    num_workers = len(compute_nodes)\n",
    "    num_model_params = len(params[compute_nodes[0]])\n",
    "\n",
    "    for data, label in train_loader:\n",
    "\n",
    "        # update remote models\n",
    "        worker = data.location.id\n",
    "        models[worker] = update(data, label, model, optimizers[worker])\n",
    "\n",
    "        # encrypted aggregation\n",
    "        updated_params = []\n",
    "        for param_idx in range(num_model_params):\n",
    "            spdz_params = []\n",
    "            for worker in compute_nodes:\n",
    "                param_i = params[worker][param_idx]\n",
    "                param_i_encrypted = param_i.fix_precision()\n",
    "                encrypted_ptr = param_i_encrypted.share(\n",
    "                    *compute_nodes, crypto_provider=carol)\n",
    "                spdz_params.append(encrypted_ptr.get())\n",
    "            updated_param = (sum(spdz_params).float_precision()) / num_workers\n",
    "            updated_params.append(updated_param)\n",
    "   \n",
    "        # clean up\n",
    "        with torch.no_grad():\n",
    "            for worker, worker_params in params.items():\n",
    "                for param_idx in range(num_model_params):\n",
    "                    worker_params[param_idx].set_(updated_params[param_idx])\n",
    "          \n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            for val_data, val_label in val_loader:\n",
    "                worker = data.location.id\n",
    "                model.send(worker)\n",
    "                log_probs = model(data)\n",
    "                loss = criterion(log_probs, label.squeeze())\n",
    "\n",
    "                curr_loss = loss.get().item()\n",
    "                val_loss += curr_loss\n",
    "                model.get()\n",
    "            \n",
    "        model.train()\n",
    "        print(\n",
    "            \"Epoch {e}/{total_e}\".format(e=epoch+1, total_e=args.num_epochs+1),\n",
    "            \"Training loss: {:.3f}.. \".format(training_loss/len(train_loader)),\n",
    "            \"Val loss: {:.3f}.. \".format(val_loss/len(val_loader)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHX7Rh13WMYJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iKaXvwhfJHuD"
   ],
   "name": "federated-ner.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
